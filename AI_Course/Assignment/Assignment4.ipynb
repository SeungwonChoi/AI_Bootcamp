{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b0d37628-d11d-43ca-bd70-44567aebbf41"
    }
   },
   "source": [
    "<H2>課題4: 評判分析</H2>\n",
    "\n",
    "<p>本課題ではAmazonに投稿された映画のレビュー(英語)を分析し、レビューがPositiveかNegativeかの判別を行います。</p>\n",
    "<p>Amazon_reviewファイルにはTraining dataとTest dataが入っています。</p>\n",
    "<p>Training dataを用いて機械学習を行い、その結果を元に6つのTest dataがpositiveかNegativeかを判別してください。</p>\n",
    "<p>6章で学んだ内容を踏まえ、各セルに'#コメント'の内容を実行するコードを記入してください。</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b14e9b2d-e81d-4c46-aaa5-a21ead865efb"
    }
   },
   "source": [
    "<H2>1. 必要なモジュールの読み込み</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import numpy as np_amazon\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn import svm, naive_bayes\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>2. ディレクトリの確認</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'Assignment1.ipynb', 'Assignment2.ipynb', 'Assignment3.ipynb', 'Assignment4.ipynb', 'dataset', 'mlbook-master', 'mlbook-master-new']\n"
     ]
    }
   ],
   "source": [
    "# カレントディレクトリの確認\n",
    "print( os.listdir(os.path.normpath(\"./\")) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Test_data', 'Training_data', '_DS_Store']\n"
     ]
    }
   ],
   "source": [
    "# データディレクトリの確認\n",
    "print( os.listdir(os.path.normpath(\"./dataset/Amazon_review\")) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>3. Dataの読み込み </H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フォルダ中のテキストへのパスを取得した上で各ファイルを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your default locale is None\n",
      "Your locale is set as ja_JP.UTF-8\n"
     ]
    }
   ],
   "source": [
    "# default locale を ja_JP.UTF-8に設定する\n",
    "def set_locale():\n",
    "    default = os.environ.get('LC_ALL')\n",
    "    print( \"Your default locale is\", default )\n",
    "    if default is None:\n",
    "        os.environ.setdefault('LC_ALL', 'ja_JP.UTF-8')\n",
    "        print( \"Your locale is set as ja_JP.UTF-8\" )\n",
    "\n",
    "set_locale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globでディレクトリを検索し、フォルダ中のテキストのパスを取得 (Training set)\n",
    "neg_files_training = glob.glob( os.path.normpath(\"./dataset/Amazon_review/Training_data/neg/*\") )\n",
    "pos_files_training = glob.glob( os.path.normpath(\"./dataset/Amazon_review/Training_data/pos/*\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset\\\\Amazon_review\\\\Training_data\\\\neg\\\\cv000_tok-9611.txt', 'dataset\\\\Amazon_review\\\\Training_data\\\\neg\\\\cv001_tok-19324.txt']\n",
      "['dataset\\\\Amazon_review\\\\Training_data\\\\pos\\\\cv000_tok-11609.txt', 'dataset\\\\Amazon_review\\\\Training_data\\\\pos\\\\cv001_tok-10180.txt']\n"
     ]
    }
   ],
   "source": [
    "# 相対パスで各テキストデータへのパスが格納されていることを確認\n",
    "print(neg_files_training[0:2])\n",
    "print(pos_files_training[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globでディレクトリを検索し、フォルダ中のテキストのパスを取得 (Test set)\n",
    "neg_files_test = glob.glob( os.path.normpath(\"./dataset/Amazon_review/Test_data/neg/*\") )\n",
    "pos_files_test = glob.glob( os.path.normpath(\"./dataset/Amazon_review/Test_data/pos/*\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset\\\\Amazon_review\\\\Test_data\\\\neg\\\\amazon_review_10000.txt', 'dataset\\\\Amazon_review\\\\Test_data\\\\neg\\\\amazon_review_10001.txt']\n",
      "['dataset\\\\Amazon_review\\\\Test_data\\\\pos\\\\amaozn_review_20000.txt', 'dataset\\\\Amazon_review\\\\Test_data\\\\pos\\\\amaozn_review_20001.txt']\n"
     ]
    }
   ],
   "source": [
    "# 相対パスで各テキストデータへのパスが格納されていることを確認\n",
    "print(neg_files_test[0:2])\n",
    "print(pos_files_test[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  各ファイルを読み込む関数を定義\n",
    "def text_reader(file_path):\n",
    "    python_version = sys.version_info.major\n",
    "    \n",
    "    if python_version >= 3:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = re.sub( r'\\*.\\*.\\*.\\*.\\*{0}+\\*.\\*.\\*.\\*{0}+\\*.\\*.\\*{0}+\\*.\\*{0}+\\*[^\\.]+\\.'.format('[^\\*]'), \"REVIEW_FORMAT\" , line )\n",
    "                line = re.sub(r'<a(.*?)</a>',\"HTML_TAG\", line)\n",
    "                print(line)\n",
    "    else:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = re.sub( r'\\*.\\*.\\*.\\*.\\*{0}+\\*.\\*.\\*.\\*{0}+\\*.\\*.\\*{0}+\\*.\\*{0}+\\*[^\\.]+\\.'.format('[^\\*]'), \"REVIEW_FORMAT\" , line )\n",
    "                line = re.sub(r'<a(.*?)</a>',\"HTML_TAG\", line)\n",
    "                print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bad: Disregard for plot points from the lead up movies, including the first Avengers (mentioned, but stripped of all layers). Characters having sudden new and unexplained personalities (bound to happen when they're juggling so damned many!). CGI is a wonderful tool, but when you see Spider-Man's head floating above his CGI body (no he did not get decapitated), it pulls you out of the narrative. The focus of the film was on too many characters who were not relevant to the central plot, maybe the excuse was to have an epic Final Battle scene in Wakanda (SPOILER: Some of those dead characters are needed for a Black Panther sequel, further proving my point).\n",
      "\n",
      "\n",
      "\n",
      "The Ugly: One character death after another, with so many dying that they often skip to the aftermath or just state that Thanos killed them, it really pulls at the heartstrings... Except, as this review title implies, the movie has zero lasting effects. They wasted no time in throwing out all credibility (SPOILER: Thor caused the extinction of the Asgardians), so the rest is just a fun What If? movie focused on committing the Woman In Fridge cliché as many characters as possible.\n"
     ]
    }
   ],
   "source": [
    "# 読み込んだファイルの中身を表示（Test data, Negative, 1ファイル目）\n",
    "text_reader(neg_files_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> 4.特徴ベクトルの作成</H2>\n",
    "<p>特徴ベクトルの作成に必要な関数を定義した後に特徴ベクトルを作成します。</p>\n",
    "<p>特徴ベクトルはTraining setとTest setで同じ要素を持つ必要がありますので、両方のsetを合わせて作成し、後でTraining setとTest setに分けます。</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各単語の出現回数をカウントする関数を定義\n",
    "def word_counter(string):\n",
    "    words = string.strip().split()\n",
    "    count_dict = collections.Counter(words)\n",
    "    return dict(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigramを作成する関数を定義\n",
    "def get_unigram(file_path):\n",
    "    result = []\n",
    "    python_version = sys.version_info.major\n",
    "    \n",
    "    if python_version >= 3:\n",
    "        for file in file_path:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                text = ''.join([line.rstrip('\\r\\n') for line in f])\n",
    "                text = re.sub( r'\\*.\\*.\\*.\\*.\\*{0}+\\*.\\*.\\*.\\*{0}+\\*.\\*.\\*{0}+\\*.\\*{0}+\\*[^\\.]+\\.'.format('[^\\*]'), \"REVIEW_FORMAT\" , text )\n",
    "                text = re.sub(r'<a(.*?)</a>',\"HTML_TAG\", text)\n",
    "                count_dict = word_counter(text)\n",
    "                result.append(count_dict)\n",
    "    else:\n",
    "        for file in file_path:\n",
    "            with open(file, 'r') as f:\n",
    "                text = ''.join([line.rstrip('\\r\\n') for line in f])\n",
    "                text = re.sub( r'\\*.\\*.\\*.\\*.\\*{0}+\\*.\\*.\\*.\\*{0}+\\*.\\*.\\*{0}+\\*.\\*{0}+\\*[^\\.]+\\.'.format('[^\\*]'), \"REVIEW_FORMAT\" , text )\n",
    "                text = re.sub(r'<a(.*?)</a>',\"HTML_TAG\", text)\n",
    "                count_dict = word_counter(text)\n",
    "                result.append(count_dict)\n",
    "    \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigramをリストアップ (Training set + Test set)\n",
    "unigrams_data = get_unigram(neg_files_training[:]) + get_unigram(pos_files_training[:]) \\\n",
    "                + get_unigram(neg_files_test[:]) + get_unigram(pos_files_test[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1406"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigrams_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shirtless': 1, 'dennis': 3, 'level': 1, 'gratuitous': 1, \"quinn's\": 1, 'mass': 1, 'namely': 1, \"who's\": 1, 'frame': 1, 'be': 2, 'antwerp': 1, 'performance': 1, 'amusement': 1, 'the': 20, 'tries': 2, 'lite': 1, 'segment': 1, \"neither's\": 1, 'this': 2, 'set': 1, 'colony': 1, 'been': 1, 'tell': 1, 'soldiers': 1, 'than': 1, 'whatever': 1, 'ripe': 1, 'figure': 1, 'world': 1, '.': 21, 'them': 2, 'one': 2, 'story': 1, 'escapes': 1, 'high': 1, 'think': 1, 'jamie': 1, 'strange': 1, '?': 3, 'action': 1, 'are': 1, 'out': 3, 'aim': 1, 'juices': 1, 'r': 1, 'from': 1, 'weird-looking': 1, 'care': 1, 'expert': 1, 'exuberantly': 1, \"he's\": 3, 'damme': 4, 'on': 5, 'mones': 1, 'numerous': 1, 'make': 1, 'lindinger': 2, 'did': 2, 'locks': 1, 'gets': 1, 'arms': 1, 'cast': 1, 'stars': 1, 'ex-cia': 1, 'taken': 1, 'tiger': 1, \"there's\": 1, 'and': 17, 'frenetic': 1, 'else': 1, 'needs': 2, 'save': 1, \"didn't\": 1, 'known': 1, 'bad': 2, 'who': 1, 'basket': 1, 'seagal': 1, 'actually': 1, 'double': 5, 'movie-bedpost': 1, 'camera': 1, 'critic': 1, 'mention': 1, 'visit': 1, 'valuable': 1, 'steven': 1, 'to': 20, 'up': 4, 'should': 2, 'wacky': 1, 'yet': 1, 'while': 2, 'superstar': 1, 'mickey': 2, 'never': 3, 'much': 4, 'gunfire': 1, 'often': 1, 'coliseum': 1, 'hark': 1, 'his': 4, 'have': 2, 'revenge': 1, 'boy': 1, 'even': 1, 'death': 1, 'lunch': 1, 'really': 1, 'formula': 1, 'free': 1, 'pretty': 2, 'peter': 1, 'well': 1, 'that': 10, 'feels': 1, 'mines': 1, 'charisma': 1, \"hark's\": 1, 'pleasure': 1, 'off': 1, 'looks': 1, 'vibrant': 1, 'made': 1, 'role': 2, 'colorful': 1, 'extremely': 1, 'HTML_TAG': 2, 'though': 1, \"1994's\": 1, 'dan': 1, 'kill': 2, 'trying': 1, 'can': 1, 'exhilarated': 1, 'slows': 1, 'screenplay': 2, 'tristar': 1, 'home': 1, 'rather': 1, 'circles': 1, 'jakoby': 1, 'scenes': 1, 'net-surfing': 1, 'do': 4, 'by': 1, 'indulge': 1, 'legs': 1, 'weird': 1, 'natacha': 2, 'quinn': 5, 'as': 1, 'running': 1, 'land': 1, 'major': 1, 'over': 1, 'weapons': 1, 'tipsy': 1, 'plays': 1, 'park': 1, 'stunt': 1, 'albeit': 1, 'son': 1, 'her': 1, 'hold': 1, 'wife': 1, 'nba': 1, 'if': 1, 'decided': 1, 'killathon': 1, 'profanity': 1, 'screen': 1, 'paul': 2, '1997': 2, 'possesses': 1, 'rarely': 1, 'dealer': 1, 'rescue': 1, 'work': 2, 'going': 3, 'operative': 1, ':': 6, 'jack': 1, 'killed': 1, 'finds': 1, 'we': 2, 'rourke': 2, 'between': 1, 'opponent': 1, 'stavros': 4, '--': 3, 'always': 1, 'they': 1, 'bleachers': 1, 'teams': 1, 'an': 5, ')': 6, 'i': 2, 'pacing': 1, 'baby': 1, 'whole': 1, 'must': 1, '1': 1, 'dangerous': 1, 'pregnant': 1, 'where': 1, 'another': 2, 'equipped': 1, 'enough': 1, \"i've\": 1, 'enjoyable': 1, 'you': 1, 'it': 2, \"it's\": 2, 'so': 3, 'just': 3, 'jean-claude': 2, 'exhausted': 1, \"stavros'\": 1, 'persona': 1, '(': 6, '\"': 18, 'result': 2, 'too': 3, ';': 6, 'deadly': 1, 'us': 2, 'leading': 1, 'beefy': 1, 'van': 4, \"what's\": 3, 'head-to-head': 1, 'since': 1, 'some': 2, 'job': 1, 'two': 1, 'hairdos': 1, 'he': 4, 'motorcycle': 1, 'of': 3, 'defend': 1, 'timecop': 1, 'eye-popping': 1, 'notch': 1, 'climax': 1, 'kicks': 1, 'language': 1, 'is': 9, 'requires': 1, 'gangster': 1, 'therefore': 1, 'blatantly': 1, 'island': 1, 'all': 4, 'avoid': 1, 'about': 1, 'movie': 3, 'tank': 1, 'yaz': 3, 'things': 1, 'watches': 1, 'rub': 1, 'roman': 1, 't': 1, 'career': 1, 'e-mail': 1, 'bomb': 1, 'botched': 1, 'woman': 1, 'pau': 1, 'explosions': 1, 'for': 4, 'entertaining': 1, 'headache-inducing': 1, 'witty': 1, 'back': 1, 'team': 5, '30': 1, 'with': 5, 'acting': 1, 'each': 1, 'rodman': 7, 'devoted': 1, 'leaves': 1, 'fits': 1, 'rome': 1, '/': 3, 'but': 4, 'deal': 1, 'violence': 1, 'need': 2, \"rodman's\": 1, 'comes': 1, 'hollywood': 1, 'power': 1, 'travel': 1, 'guilty': 1, 'crazy': 1, 'peck': 1, 'genre': 1, 'brite': 1, 'online': 1, 'a': 13, 'mildly': 1, 'kidnaps': 1, 'monks': 1, 'happen': 1, 'down': 2, 'freeman': 1, 'mercenary': 1, 'director': 1, ',': 25, 'when': 2, 'kickboxing': 1, 'in': 8, 'no': 1, 'loud': 1, 'could': 1, 'reel': 1, 'counter-terrorist': 1, 'tsui': 2}\n"
     ]
    }
   ],
   "source": [
    "# Unigramの内容を確認 (Training set)\n",
    "print( unigrams_data[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DictVectorizerを用いてUnigramを行列の形に変形　（各行が1つのレビュー、各列が単語、要素がその単語の出現数）\n",
    "vec = DictVectorizer()\n",
    "feature_vectors_csr = vec.fit_transform( unigrams_data )\n",
    "feature_vectors = vec.fit_transform( unigrams_data ).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1406x44266 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 492109 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vectors_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension : (1406, 44266)\n",
      "[ 0.  0. 18. ...  0.  0.  0.]\n",
      "data size : 497.90408 [MB]\n"
     ]
    }
   ],
   "source": [
    "# 作成したデータを確認\n",
    "print( \"data dimension :\", feature_vectors.shape )\n",
    "print( feature_vectors[0] )\n",
    "print( \"data size :\", sys.getsizeof(feature_vectors) / 1000000, \"[MB]\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.ラベルデータの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回扱うデータセットは全てに negative, positive というラベルが振られています。<br>\n",
    "ここではそのラベルを negative → 0, positive → 1 とすることで二値判別問題のセットアップを構築します。<br>\n",
    "ラベルも特徴ベクトルと同様にTraining setとTest setを合わせて作成し、後でTraining setとTest　setに分けます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ラベルデータの作成\n",
    "labels = np.r_[np.tile(0, 700), np.tile(1, 700), np.tile(0, 3), np.tile(1, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1 1 0 0 1 1\n"
     ]
    }
   ],
   "source": [
    "#正しい位置で0と1の振替がなされているか確認 (Training setの0と1、Test setの0と1)\n",
    "print( labels[0], labels[699], labels[700], labels[1399], \\\n",
    "      labels[1400], labels[1402], labels[1403], labels[1405]  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> 6. 学習用データと学習評価用データの作成</H2>\n",
    "<p>今回はTree fold cross validationを行いますので、まずTraining setを3分割します。</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回はThree fold cross validation でモデルを評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seedを固定し、乱数を1400個作成\n",
    "np.random.seed(1010)\n",
    "shuffle_order = np.random.choice( 1400, 1400, replace=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length : 1400\n",
      "first 10 elements : [ 255 1231  979  696  760  238 1209  316 1008  605]\n"
     ]
    }
   ],
   "source": [
    "#生成した乱数の中身を確認\n",
    "print( \"length :\", len(shuffle_order) )\n",
    "print( \"first 10 elements :\", shuffle_order[0:10] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one third of the length : 466\n",
      "# of '1' in 1st set : 225\n",
      "# of '1' in 2nd set : 235\n",
      "# of '1' in 3rd set : 240\n"
     ]
    }
   ],
   "source": [
    "#作成した乱数を元にTraining setを3分割する\n",
    "one_third_size = int( 1400 / 3. )\n",
    "print( \"one third of the length :\", one_third_size )\n",
    "\n",
    "print( \"# of '1' in 1st set :\", np.sum( labels[ shuffle_order[:one_third_size] ]  ) )\n",
    "print( \"# of '1' in 2nd set :\", np.sum( labels[ shuffle_order[one_third_size:2*one_third_size] ]  ) )\n",
    "print( \"# of '1' in 3rd set :\", np.sum( labels[ shuffle_order[2*one_third_size:] ]  ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 学習に必要な関数の設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 与えられたリストをN分割する関数<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与えられたリストをN分割する関数を定義\n",
    "def N_splitter(seq, N):\n",
    "    avg = len(seq) / float(N)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "    \n",
    "    while last < len(seq):\n",
    "        out.append( seq[int(last):int(last + avg)] )\n",
    "        last += avg\n",
    "        \n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([range(0, 3), range(3, 7), range(7, 11)], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 動作を確認\n",
    "N_splitter(range(11), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>2. train_model : 説明変数とラベルと手法を与えることでモデルを学習する</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model \n",
    "def train_model(features, labels, method='SVM', parameters=None):\n",
    "    # モデル指定\n",
    "    if method == 'SVM':\n",
    "        model = svm.SVC()\n",
    "    elif method == 'NB':\n",
    "        model = naive_bayes.GaussianNB()\n",
    "    elif method == 'RF':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        print(\"Set method as SVM (for Support vector machine), NB (for Naive Bayes) or RF (Random Forest)\")\n",
    "    # パラメータがあれば指定\n",
    "    if parameters:\n",
    "        model.set_params(**parameters)\n",
    "    # モデルを学習させる\n",
    "    model.fit( features, labels )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>3. predict : モデルと説明変数を与えることでラベルを予測する</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict \n",
    "def predict(model, features):\n",
    "    predictions = model.predict( features )\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>4. evaluate_model : 予測したラベルと実際の答えの合致数を調べる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_model \n",
    "def evaluate_model(predictions, labels):\n",
    "    data_num = len(labels)\n",
    "    correct_num = np.sum( predictions == labels )\n",
    "    return data_num, correct_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> 5. cross_validate : cross_validationを実行する </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validate\n",
    "def cross_validate(n_folds, feature_vectors, labels, shuffle_order, method='SVM', parameters=None):\n",
    "    result_test_num = []\n",
    "    result_correct_num = []\n",
    "    \n",
    "    n_splits = N_splitter( range(1400), n_folds )\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        print( \"Executing {0}th set...\".format(i+1) )\n",
    "        \n",
    "        test_elems = shuffle_order[ n_splits[i] ]\n",
    "        train_elems = np.array([])\n",
    "        train_set = n_splits[ np.arange(n_folds) !=i ]\n",
    "        for j in train_set:\n",
    "            train_elems = np.r_[ train_elems, shuffle_order[j] ]\n",
    "        train_elems = train_elems.astype(np.integer)\n",
    "\n",
    "        # 学習\n",
    "        model = train_model( feature_vectors[train_elems], labels[train_elems], method, parameters )\n",
    "        # 予測\n",
    "        predictions = predict( model, feature_vectors[test_elems] )\n",
    "        # 評価\n",
    "        test_num, correct_num = evaluate_model( predictions, labels[test_elems] )\n",
    "        result_test_num.append( test_num )\n",
    "        result_correct_num.append( correct_num )\n",
    "    \n",
    "    return result_test_num, result_correct_num ,model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 学習の実施、精度の検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Trainingを実施\n",
    "ans,corr,model = cross_validate(3, feature_vectors_csr, labels, shuffle_order, method='SVM', parameters=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision :  63.9 %\n"
     ]
    }
   ],
   "source": [
    "# 結果を表示\n",
    "print( \"average precision : \", np.around( 100.*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. パラメータのチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#パラメータをチューニング（どのような形でも構いません）\n",
    "feature_vectors_csr.data[ feature_vectors_csr.data > 0 ] = 1.\n",
    "\n",
    "search_parameters = [\n",
    "    {'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4], 'C': [0.1, 1, 10, 100, 1000]},\n",
    "    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000]}\n",
    "]\n",
    "\n",
    "model = svm.SVC()\n",
    "clf = GridSearchCV(model, search_parameters)\n",
    "clf.fit( feature_vectors_csr, labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best paremters :  {'gamma': 0.001, 'C': 10, 'kernel': 'rbf'}\n",
      "best scores :  0.8193456614509246\n"
     ]
    }
   ],
   "source": [
    "# 結果を表示\n",
    "print(\"best paremters : \", clf.best_params_)\n",
    "print(\"best scores : \", clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# チューニング後のパラメータで学習\n",
    "ans,corr,model = cross_validate(3, feature_vectors_csr, labels, shuffle_order, method='SVM', parameters=clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision :  81.7 %\n"
     ]
    }
   ],
   "source": [
    "# 結果を表示\n",
    "print( \"average precision : \", np.around( 100.*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test dataを用いてモデルを評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上記で作成したモデルを用いてTest dataの評価を実施\n",
    "ans,corr = evaluate_model( predict( model, feature_vectors_csr[1400:] ), labels[1400:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision :  66.7 %\n"
     ]
    }
   ],
   "source": [
    "# 結果を表示\n",
    "print( \"average precision : \", np.around( 100.*corr/ans, decimals=1 ), \"%\" )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
